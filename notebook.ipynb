{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIGNAL PROCESS IDENTIFICATION",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Signal Process Identification and Verification of the interpolatable property of Parameterised Neural Networks"
      ],
      "metadata": {
        "id": "3YcOz85uoVu5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HzTg1Sp1IzjS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assigning the path of training data(without mass=1000), testing data(without mass=1000) and testing data with mass=1000.\n",
        "\n",
        "---\n",
        "The train_data_path contains data with masses from the set {500, 750, 1250, 1500}.\n",
        "\n",
        "---\n",
        "The test_data_path contains testing data with masses from the set {500, 750, 1250, 1500}.\n",
        "\n",
        "---\n",
        "\n",
        "The test1000_data_path contains data for mass=1000.\n",
        "\n",
        "---\n",
        "We shall train our Neural Network model only for the data with masses from the set {500, 750, 1250, 1500}. But as we shall see later on, when we test on data with mass=1000, our neural network model successfully interpolates and performs a good classification between collisions which produce new particles and collsions which do not.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mPz6kR1_eogA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One can change the paths of the respective data if needed\n",
        "\n",
        "train_data_path = '/content/drive/MyDrive/collision_data/not1000_train.csv.gz'\n",
        "test_data_path = '/content/drive/MyDrive/collision_data/not1000_test.csv.gz'\n",
        "test1000_data_path = '/content/drive/MyDrive/collision_data/1000_test.csv.gz'"
      ],
      "metadata": {
        "id": "D3aYRVAkLCgR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the training data as numpy array. The first row contains the column labels, hence it is skipped."
      ],
      "metadata": {
        "id": "mqM2eXaUfDFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt(train_data_path, delimiter=',', skiprows=1, dtype=np.float32)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXTGKWVofu2z",
        "outputId": "e5c6d80a-0d9b-4e60-c0d0-6de2e7aa729a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7000000, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the training data into features and labels"
      ],
      "metadata": {
        "id": "0z9Co_PSfSKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_numpy = data[:, 1:]\n",
        "y_numpy = data[:, 0]"
      ],
      "metadata": {
        "id": "aq0cskP7hZxB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for the balance of the labels 0 and 1. As we can see, the label classes are pretty much balanced. The frequency of each label is nearly same."
      ],
      "metadata": {
        "id": "xZyplKVPfXZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(y_numpy == 1).sum() / y_numpy.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUURyByBhmih",
        "outputId": "d5add53c-9062-40bb-cefc-c7b1617f3b1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5001255714285714"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthur dividing the training data into two parts for training and validation. 20% of the training data is kept aside for validtion. We shall train on the remaining 80% of the data."
      ],
      "metadata": {
        "id": "gNEN2GVXf2Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_numpy_train, X_numpy_valid, y_numpy_train, y_numpy_valid = train_test_split(X_numpy, y_numpy, test_size=0.2, random_state=2)"
      ],
      "metadata": {
        "id": "6cPMVVCFh53G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_numpy_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNdGpbPUj3f4",
        "outputId": "470d3994-8e47-45e4-8960-65e397703967"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5600000, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are in total 28 features. The last feature is the mass of the new particle produced for the collision sample data. All the 27 features except mass, are normalised. So we only need to scale the mass feature. We use sklearns's Standard Scaler to standard scale the mass feature."
      ],
      "metadata": {
        "id": "kgMMm4U5gG81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mass_scaler = StandardScaler()\n",
        "\n",
        "training_masses = X_numpy_train[:, 27].reshape(-1, 1)\n",
        "validation_masses = X_numpy_valid[:, 27].reshape(-1, 1)\n",
        "\n",
        "training_masses = mass_scaler.fit_transform(training_masses)\n",
        "validation_masses = mass_scaler.transform(validation_masses)\n",
        "\n",
        "X_numpy_train[:, 27] = training_masses.flatten()\n",
        "X_numpy_valid[:, 27] = validation_masses.flatten()"
      ],
      "metadata": {
        "id": "dbIuRmfajiho"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom Dataset class for training and validation. The train variable in the constructor acts as a flag. If True, then an instance of the training data will be returned. If False, then an instance of the validation data will be returned."
      ],
      "metadata": {
        "id": "UCV0BOsfg2cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CollisionData(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        self.n_samples = y_numpy_train.shape[0] if train else y_numpy_valid.shape[0]\n",
        "        \n",
        "        if train:\n",
        "            self.X = torch.from_numpy(X_numpy_train)\n",
        "            self.y = torch.from_numpy(y_numpy_train).view(self.n_samples, 1)\n",
        "        else:\n",
        "            self.X = torch.from_numpy(X_numpy_valid)\n",
        "            self.y = torch.from_numpy(y_numpy_valid).view(self.n_samples, 1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "Ls2c_V8SKkdY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collision_data = CollisionData()"
      ],
      "metadata": {
        "id": "Jps4gnqUVsS6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(collision_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK-yiQI4Vyde",
        "outputId": "d4f0cf0a-e992-4f12-a0f1-975319e2fb81"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5600000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom model by inheriting from the superclass nn.Module. The constructor takes in a list of layers for the neural network. Upon calling forward, the input is forward propagated across all the layers. In the end the sigmoid activation is applied and the output is returned."
      ],
      "metadata": {
        "id": "IjqN5GN9hkkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layers = layers\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "dUEIhL4zfNgl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = X_numpy.shape[1]"
      ],
      "metadata": {
        "id": "VDA3yWPTn9OQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the layers for our model, and creating an instance of the model with the same layers. The layers are:\n",
        "\n",
        "\n",
        "1.   Linear Layer with 28 input neurons(number of features) and 128 output neurons.\n",
        "2.   ReLU activation function.\n",
        "\n",
        "1.   Linear Layer with 128 input neurons and 256 output neurons.\n",
        "2.   ReLU activation function.\n",
        "\n",
        "1.   Linear Layer with 256 input neurons and 1 output neuron.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u2VN7OwqiLQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [nn.Linear(n_features, 128), nn.ReLU(), nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 1)]\n",
        "model = NeuralNetwork(layers)"
      ],
      "metadata": {
        "id": "SGYJ3--Gn4BS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below returns the set of all parameters in our model, which are essentially the weights in the linear layers. The derivative of loss with respect to these weights will be calculated in each iteration while training, and the weights will be updated accordingly."
      ],
      "metadata": {
        "id": "htz_Tblejb7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(layers):\n",
        "    params = []\n",
        "    for layer in layers:\n",
        "        params += layer.parameters()\n",
        "    return params"
      ],
      "metadata": {
        "id": "YmKm2rz7rHLL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train our model by providing batches of training data with the batch size being 1000. Since we are performing binary classification, the loss function used is Binary Cross Entropy Loss. The optimiser being used is the stochastic gradient descent."
      ],
      "metadata": {
        "id": "U2AVifHzj7Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "batch_size = 1000\n",
        "criterion = nn.BCELoss()\n",
        "optimiser = torch.optim.SGD(get_params(model.layers), lr=learning_rate)"
      ],
      "metadata": {
        "id": "hql6QNwcnHi0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a DataLoader instance for the training data. It shall provide batches of training data one after the other for training."
      ],
      "metadata": {
        "id": "TUY0s07Xlzkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = collision_data, batch_size=batch_size, shuffle=True)\n",
        "total_iters = len(train_loader)"
      ],
      "metadata": {
        "id": "1KJGLTZ8r_mh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    for i, (features, true_labels) in enumerate(train_loader):\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, true_labels)\n",
        "\n",
        "        optimiser.zero_grad() # We calculate new set of gradients for each iteration\n",
        "        loss.backward()       # Calculate gradients\n",
        "        optimiser.step()      # Update weights(parameters)\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'epoch: {epoch+1}/{epochs}. iter: {i+1}/{total_iters}. loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L01HZ3ze0HqB",
        "outputId": "8d59351b-b883-40ed-91e2-00148235da74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1/10. iter: 100/5600. loss: 0.5900\n",
            "epoch: 1/10. iter: 200/5600. loss: 0.5367\n",
            "epoch: 1/10. iter: 300/5600. loss: 0.4932\n",
            "epoch: 1/10. iter: 400/5600. loss: 0.4593\n",
            "epoch: 1/10. iter: 500/5600. loss: 0.4688\n",
            "epoch: 1/10. iter: 600/5600. loss: 0.4324\n",
            "epoch: 1/10. iter: 700/5600. loss: 0.4142\n",
            "epoch: 1/10. iter: 800/5600. loss: 0.3884\n",
            "epoch: 1/10. iter: 900/5600. loss: 0.3971\n",
            "epoch: 1/10. iter: 1000/5600. loss: 0.3954\n",
            "epoch: 1/10. iter: 1100/5600. loss: 0.4079\n",
            "epoch: 1/10. iter: 1200/5600. loss: 0.4258\n",
            "epoch: 1/10. iter: 1300/5600. loss: 0.3641\n",
            "epoch: 1/10. iter: 1400/5600. loss: 0.3565\n",
            "epoch: 1/10. iter: 1500/5600. loss: 0.4132\n",
            "epoch: 1/10. iter: 1600/5600. loss: 0.3837\n",
            "epoch: 1/10. iter: 1700/5600. loss: 0.3742\n",
            "epoch: 1/10. iter: 1800/5600. loss: 0.3875\n",
            "epoch: 1/10. iter: 1900/5600. loss: 0.3755\n",
            "epoch: 1/10. iter: 2000/5600. loss: 0.3561\n",
            "epoch: 1/10. iter: 2100/5600. loss: 0.3567\n",
            "epoch: 1/10. iter: 2200/5600. loss: 0.4226\n",
            "epoch: 1/10. iter: 2300/5600. loss: 0.3469\n",
            "epoch: 1/10. iter: 2400/5600. loss: 0.3780\n",
            "epoch: 1/10. iter: 2500/5600. loss: 0.3701\n",
            "epoch: 1/10. iter: 2600/5600. loss: 0.3714\n",
            "epoch: 1/10. iter: 2700/5600. loss: 0.3613\n",
            "epoch: 1/10. iter: 2800/5600. loss: 0.3897\n",
            "epoch: 1/10. iter: 2900/5600. loss: 0.3566\n",
            "epoch: 1/10. iter: 3000/5600. loss: 0.3635\n",
            "epoch: 1/10. iter: 3100/5600. loss: 0.3592\n",
            "epoch: 1/10. iter: 3200/5600. loss: 0.3834\n",
            "epoch: 1/10. iter: 3300/5600. loss: 0.3749\n",
            "epoch: 1/10. iter: 3400/5600. loss: 0.3646\n",
            "epoch: 1/10. iter: 3500/5600. loss: 0.3644\n",
            "epoch: 1/10. iter: 3600/5600. loss: 0.3750\n",
            "epoch: 1/10. iter: 3700/5600. loss: 0.3457\n",
            "epoch: 1/10. iter: 3800/5600. loss: 0.3873\n",
            "epoch: 1/10. iter: 3900/5600. loss: 0.3361\n",
            "epoch: 1/10. iter: 4000/5600. loss: 0.3248\n",
            "epoch: 1/10. iter: 4100/5600. loss: 0.3732\n",
            "epoch: 1/10. iter: 4200/5600. loss: 0.3755\n",
            "epoch: 1/10. iter: 4300/5600. loss: 0.3670\n",
            "epoch: 1/10. iter: 4400/5600. loss: 0.3799\n",
            "epoch: 1/10. iter: 4500/5600. loss: 0.3566\n",
            "epoch: 1/10. iter: 4600/5600. loss: 0.3577\n",
            "epoch: 1/10. iter: 4700/5600. loss: 0.3409\n",
            "epoch: 1/10. iter: 4800/5600. loss: 0.3370\n",
            "epoch: 1/10. iter: 4900/5600. loss: 0.3509\n",
            "epoch: 1/10. iter: 5000/5600. loss: 0.3655\n",
            "epoch: 1/10. iter: 5100/5600. loss: 0.3316\n",
            "epoch: 1/10. iter: 5200/5600. loss: 0.3558\n",
            "epoch: 1/10. iter: 5300/5600. loss: 0.3598\n",
            "epoch: 1/10. iter: 5400/5600. loss: 0.3518\n",
            "epoch: 1/10. iter: 5500/5600. loss: 0.3478\n",
            "epoch: 1/10. iter: 5600/5600. loss: 0.3417\n",
            "epoch: 2/10. iter: 100/5600. loss: 0.3553\n",
            "epoch: 2/10. iter: 200/5600. loss: 0.3411\n",
            "epoch: 2/10. iter: 300/5600. loss: 0.3741\n",
            "epoch: 2/10. iter: 400/5600. loss: 0.3692\n",
            "epoch: 2/10. iter: 500/5600. loss: 0.3450\n",
            "epoch: 2/10. iter: 600/5600. loss: 0.3478\n",
            "epoch: 2/10. iter: 700/5600. loss: 0.3258\n",
            "epoch: 2/10. iter: 800/5600. loss: 0.3536\n",
            "epoch: 2/10. iter: 900/5600. loss: 0.3266\n",
            "epoch: 2/10. iter: 1000/5600. loss: 0.3314\n",
            "epoch: 2/10. iter: 1100/5600. loss: 0.3599\n",
            "epoch: 2/10. iter: 1200/5600. loss: 0.3588\n",
            "epoch: 2/10. iter: 1300/5600. loss: 0.3249\n",
            "epoch: 2/10. iter: 1400/5600. loss: 0.3450\n",
            "epoch: 2/10. iter: 1500/5600. loss: 0.3639\n",
            "epoch: 2/10. iter: 1600/5600. loss: 0.3492\n",
            "epoch: 2/10. iter: 1700/5600. loss: 0.3543\n",
            "epoch: 2/10. iter: 1800/5600. loss: 0.3550\n",
            "epoch: 2/10. iter: 1900/5600. loss: 0.3641\n",
            "epoch: 2/10. iter: 2000/5600. loss: 0.3326\n",
            "epoch: 2/10. iter: 2100/5600. loss: 0.3540\n",
            "epoch: 2/10. iter: 2200/5600. loss: 0.3322\n",
            "epoch: 2/10. iter: 2300/5600. loss: 0.3194\n",
            "epoch: 2/10. iter: 2400/5600. loss: 0.3421\n",
            "epoch: 2/10. iter: 2500/5600. loss: 0.3419\n",
            "epoch: 2/10. iter: 2600/5600. loss: 0.3168\n",
            "epoch: 2/10. iter: 2700/5600. loss: 0.3405\n",
            "epoch: 2/10. iter: 2800/5600. loss: 0.3331\n",
            "epoch: 2/10. iter: 2900/5600. loss: 0.3283\n",
            "epoch: 2/10. iter: 3000/5600. loss: 0.3590\n",
            "epoch: 2/10. iter: 3100/5600. loss: 0.3426\n",
            "epoch: 2/10. iter: 3200/5600. loss: 0.3295\n",
            "epoch: 2/10. iter: 3300/5600. loss: 0.3288\n",
            "epoch: 2/10. iter: 3400/5600. loss: 0.3167\n",
            "epoch: 2/10. iter: 3500/5600. loss: 0.3468\n",
            "epoch: 2/10. iter: 3600/5600. loss: 0.3580\n",
            "epoch: 2/10. iter: 3700/5600. loss: 0.3685\n",
            "epoch: 2/10. iter: 3800/5600. loss: 0.3258\n",
            "epoch: 2/10. iter: 3900/5600. loss: 0.3585\n",
            "epoch: 2/10. iter: 4000/5600. loss: 0.3382\n",
            "epoch: 2/10. iter: 4100/5600. loss: 0.3841\n",
            "epoch: 2/10. iter: 4200/5600. loss: 0.3447\n",
            "epoch: 2/10. iter: 4300/5600. loss: 0.3600\n",
            "epoch: 2/10. iter: 4400/5600. loss: 0.3265\n",
            "epoch: 2/10. iter: 4500/5600. loss: 0.3035\n",
            "epoch: 2/10. iter: 4600/5600. loss: 0.3144\n",
            "epoch: 2/10. iter: 4700/5600. loss: 0.3205\n",
            "epoch: 2/10. iter: 4800/5600. loss: 0.3657\n",
            "epoch: 2/10. iter: 4900/5600. loss: 0.3587\n",
            "epoch: 2/10. iter: 5000/5600. loss: 0.3049\n",
            "epoch: 2/10. iter: 5100/5600. loss: 0.3524\n",
            "epoch: 2/10. iter: 5200/5600. loss: 0.3222\n",
            "epoch: 2/10. iter: 5300/5600. loss: 0.3305\n",
            "epoch: 2/10. iter: 5400/5600. loss: 0.3377\n",
            "epoch: 2/10. iter: 5500/5600. loss: 0.3757\n",
            "epoch: 2/10. iter: 5600/5600. loss: 0.3475\n",
            "epoch: 3/10. iter: 100/5600. loss: 0.3558\n",
            "epoch: 3/10. iter: 200/5600. loss: 0.3534\n",
            "epoch: 3/10. iter: 300/5600. loss: 0.3431\n",
            "epoch: 3/10. iter: 400/5600. loss: 0.3084\n",
            "epoch: 3/10. iter: 500/5600. loss: 0.3340\n",
            "epoch: 3/10. iter: 600/5600. loss: 0.3407\n",
            "epoch: 3/10. iter: 700/5600. loss: 0.3417\n",
            "epoch: 3/10. iter: 800/5600. loss: 0.3576\n",
            "epoch: 3/10. iter: 900/5600. loss: 0.3384\n",
            "epoch: 3/10. iter: 1000/5600. loss: 0.3227\n",
            "epoch: 3/10. iter: 1100/5600. loss: 0.3416\n",
            "epoch: 3/10. iter: 1200/5600. loss: 0.3369\n",
            "epoch: 3/10. iter: 1300/5600. loss: 0.3239\n",
            "epoch: 3/10. iter: 1400/5600. loss: 0.3444\n",
            "epoch: 3/10. iter: 1500/5600. loss: 0.3343\n",
            "epoch: 3/10. iter: 1600/5600. loss: 0.3312\n",
            "epoch: 3/10. iter: 1700/5600. loss: 0.3232\n",
            "epoch: 3/10. iter: 1800/5600. loss: 0.3500\n",
            "epoch: 3/10. iter: 1900/5600. loss: 0.3343\n",
            "epoch: 3/10. iter: 2000/5600. loss: 0.3174\n",
            "epoch: 3/10. iter: 2100/5600. loss: 0.3328\n",
            "epoch: 3/10. iter: 2200/5600. loss: 0.3494\n",
            "epoch: 3/10. iter: 2300/5600. loss: 0.3516\n",
            "epoch: 3/10. iter: 2400/5600. loss: 0.3318\n",
            "epoch: 3/10. iter: 2500/5600. loss: 0.3398\n",
            "epoch: 3/10. iter: 2600/5600. loss: 0.3438\n",
            "epoch: 3/10. iter: 2700/5600. loss: 0.3120\n",
            "epoch: 3/10. iter: 2800/5600. loss: 0.3376\n",
            "epoch: 3/10. iter: 2900/5600. loss: 0.3364\n",
            "epoch: 3/10. iter: 3000/5600. loss: 0.3516\n",
            "epoch: 3/10. iter: 3100/5600. loss: 0.3247\n",
            "epoch: 3/10. iter: 3200/5600. loss: 0.3182\n",
            "epoch: 3/10. iter: 3300/5600. loss: 0.3284\n",
            "epoch: 3/10. iter: 3400/5600. loss: 0.3603\n",
            "epoch: 3/10. iter: 3500/5600. loss: 0.3204\n",
            "epoch: 3/10. iter: 3600/5600. loss: 0.3179\n",
            "epoch: 3/10. iter: 3700/5600. loss: 0.3387\n",
            "epoch: 3/10. iter: 3800/5600. loss: 0.3685\n",
            "epoch: 3/10. iter: 3900/5600. loss: 0.3114\n",
            "epoch: 3/10. iter: 4000/5600. loss: 0.3479\n",
            "epoch: 3/10. iter: 4100/5600. loss: 0.3309\n",
            "epoch: 3/10. iter: 4200/5600. loss: 0.3323\n",
            "epoch: 3/10. iter: 4300/5600. loss: 0.3220\n",
            "epoch: 3/10. iter: 4400/5600. loss: 0.3247\n",
            "epoch: 3/10. iter: 4500/5600. loss: 0.3253\n",
            "epoch: 3/10. iter: 4600/5600. loss: 0.3322\n",
            "epoch: 3/10. iter: 4700/5600. loss: 0.3290\n",
            "epoch: 3/10. iter: 4800/5600. loss: 0.3225\n",
            "epoch: 3/10. iter: 4900/5600. loss: 0.3219\n",
            "epoch: 3/10. iter: 5000/5600. loss: 0.3480\n",
            "epoch: 3/10. iter: 5100/5600. loss: 0.3126\n",
            "epoch: 3/10. iter: 5200/5600. loss: 0.3340\n",
            "epoch: 3/10. iter: 5300/5600. loss: 0.3291\n",
            "epoch: 3/10. iter: 5400/5600. loss: 0.3399\n",
            "epoch: 3/10. iter: 5500/5600. loss: 0.3307\n",
            "epoch: 3/10. iter: 5600/5600. loss: 0.3192\n",
            "epoch: 4/10. iter: 100/5600. loss: 0.3168\n",
            "epoch: 4/10. iter: 200/5600. loss: 0.3424\n",
            "epoch: 4/10. iter: 300/5600. loss: 0.3439\n",
            "epoch: 4/10. iter: 400/5600. loss: 0.3215\n",
            "epoch: 4/10. iter: 500/5600. loss: 0.3144\n",
            "epoch: 4/10. iter: 600/5600. loss: 0.3384\n",
            "epoch: 4/10. iter: 700/5600. loss: 0.3416\n",
            "epoch: 4/10. iter: 800/5600. loss: 0.3257\n",
            "epoch: 4/10. iter: 900/5600. loss: 0.3462\n",
            "epoch: 4/10. iter: 1000/5600. loss: 0.3595\n",
            "epoch: 4/10. iter: 1100/5600. loss: 0.3449\n",
            "epoch: 4/10. iter: 1200/5600. loss: 0.3203\n",
            "epoch: 4/10. iter: 1300/5600. loss: 0.3573\n",
            "epoch: 4/10. iter: 1400/5600. loss: 0.3211\n",
            "epoch: 4/10. iter: 1500/5600. loss: 0.3254\n",
            "epoch: 4/10. iter: 1600/5600. loss: 0.3489\n",
            "epoch: 4/10. iter: 1700/5600. loss: 0.3554\n",
            "epoch: 4/10. iter: 1800/5600. loss: 0.3389\n",
            "epoch: 4/10. iter: 1900/5600. loss: 0.3187\n",
            "epoch: 4/10. iter: 2000/5600. loss: 0.3192\n",
            "epoch: 4/10. iter: 2100/5600. loss: 0.3362\n",
            "epoch: 4/10. iter: 2200/5600. loss: 0.3185\n",
            "epoch: 4/10. iter: 2300/5600. loss: 0.3304\n",
            "epoch: 4/10. iter: 2400/5600. loss: 0.3305\n",
            "epoch: 4/10. iter: 2500/5600. loss: 0.3305\n",
            "epoch: 4/10. iter: 2600/5600. loss: 0.3068\n",
            "epoch: 4/10. iter: 2700/5600. loss: 0.3081\n",
            "epoch: 4/10. iter: 2800/5600. loss: 0.3298\n",
            "epoch: 4/10. iter: 2900/5600. loss: 0.3376\n",
            "epoch: 4/10. iter: 3000/5600. loss: 0.3131\n",
            "epoch: 4/10. iter: 3100/5600. loss: 0.3065\n",
            "epoch: 4/10. iter: 3200/5600. loss: 0.3566\n",
            "epoch: 4/10. iter: 3300/5600. loss: 0.3419\n",
            "epoch: 4/10. iter: 3400/5600. loss: 0.3429\n",
            "epoch: 4/10. iter: 3500/5600. loss: 0.3306\n",
            "epoch: 4/10. iter: 3600/5600. loss: 0.2886\n",
            "epoch: 4/10. iter: 3700/5600. loss: 0.3243\n",
            "epoch: 4/10. iter: 3800/5600. loss: 0.3172\n",
            "epoch: 4/10. iter: 3900/5600. loss: 0.3310\n",
            "epoch: 4/10. iter: 4000/5600. loss: 0.3073\n",
            "epoch: 4/10. iter: 4100/5600. loss: 0.3372\n",
            "epoch: 4/10. iter: 4200/5600. loss: 0.3069\n",
            "epoch: 4/10. iter: 4300/5600. loss: 0.3056\n",
            "epoch: 4/10. iter: 4400/5600. loss: 0.3151\n",
            "epoch: 4/10. iter: 4500/5600. loss: 0.3365\n",
            "epoch: 4/10. iter: 4600/5600. loss: 0.3367\n",
            "epoch: 4/10. iter: 4700/5600. loss: 0.3236\n",
            "epoch: 4/10. iter: 4800/5600. loss: 0.3155\n",
            "epoch: 4/10. iter: 4900/5600. loss: 0.3298\n",
            "epoch: 4/10. iter: 5000/5600. loss: 0.3212\n",
            "epoch: 4/10. iter: 5100/5600. loss: 0.3718\n",
            "epoch: 4/10. iter: 5200/5600. loss: 0.3191\n",
            "epoch: 4/10. iter: 5300/5600. loss: 0.3561\n",
            "epoch: 4/10. iter: 5400/5600. loss: 0.3333\n",
            "epoch: 4/10. iter: 5500/5600. loss: 0.3412\n",
            "epoch: 4/10. iter: 5600/5600. loss: 0.2908\n",
            "epoch: 5/10. iter: 100/5600. loss: 0.3197\n",
            "epoch: 5/10. iter: 200/5600. loss: 0.3004\n",
            "epoch: 5/10. iter: 300/5600. loss: 0.3178\n",
            "epoch: 5/10. iter: 400/5600. loss: 0.3302\n",
            "epoch: 5/10. iter: 500/5600. loss: 0.3376\n",
            "epoch: 5/10. iter: 600/5600. loss: 0.3128\n",
            "epoch: 5/10. iter: 700/5600. loss: 0.3330\n",
            "epoch: 5/10. iter: 800/5600. loss: 0.3464\n",
            "epoch: 5/10. iter: 900/5600. loss: 0.3782\n",
            "epoch: 5/10. iter: 1000/5600. loss: 0.3253\n",
            "epoch: 5/10. iter: 1100/5600. loss: 0.3327\n",
            "epoch: 5/10. iter: 1200/5600. loss: 0.3122\n",
            "epoch: 5/10. iter: 1300/5600. loss: 0.3402\n",
            "epoch: 5/10. iter: 1400/5600. loss: 0.3269\n",
            "epoch: 5/10. iter: 1500/5600. loss: 0.3137\n",
            "epoch: 5/10. iter: 1600/5600. loss: 0.2978\n",
            "epoch: 5/10. iter: 1700/5600. loss: 0.3328\n",
            "epoch: 5/10. iter: 1800/5600. loss: 0.3242\n",
            "epoch: 5/10. iter: 1900/5600. loss: 0.3275\n",
            "epoch: 5/10. iter: 2000/5600. loss: 0.3529\n",
            "epoch: 5/10. iter: 2100/5600. loss: 0.3425\n",
            "epoch: 5/10. iter: 2200/5600. loss: 0.3313\n",
            "epoch: 5/10. iter: 2300/5600. loss: 0.3218\n",
            "epoch: 5/10. iter: 2400/5600. loss: 0.3145\n",
            "epoch: 5/10. iter: 2500/5600. loss: 0.3342\n",
            "epoch: 5/10. iter: 2600/5600. loss: 0.3420\n",
            "epoch: 5/10. iter: 2700/5600. loss: 0.3456\n",
            "epoch: 5/10. iter: 2800/5600. loss: 0.3074\n",
            "epoch: 5/10. iter: 2900/5600. loss: 0.3335\n",
            "epoch: 5/10. iter: 3000/5600. loss: 0.2995\n",
            "epoch: 5/10. iter: 3100/5600. loss: 0.3129\n",
            "epoch: 5/10. iter: 3200/5600. loss: 0.3167\n",
            "epoch: 5/10. iter: 3300/5600. loss: 0.3300\n",
            "epoch: 5/10. iter: 3400/5600. loss: 0.3248\n",
            "epoch: 5/10. iter: 3500/5600. loss: 0.3607\n",
            "epoch: 5/10. iter: 3600/5600. loss: 0.3337\n",
            "epoch: 5/10. iter: 3700/5600. loss: 0.3277\n",
            "epoch: 5/10. iter: 3800/5600. loss: 0.3160\n",
            "epoch: 5/10. iter: 3900/5600. loss: 0.3251\n",
            "epoch: 5/10. iter: 4000/5600. loss: 0.3170\n",
            "epoch: 5/10. iter: 4100/5600. loss: 0.3138\n",
            "epoch: 5/10. iter: 4200/5600. loss: 0.3165\n",
            "epoch: 5/10. iter: 4300/5600. loss: 0.3278\n",
            "epoch: 5/10. iter: 4400/5600. loss: 0.3196\n",
            "epoch: 5/10. iter: 4500/5600. loss: 0.3518\n",
            "epoch: 5/10. iter: 4600/5600. loss: 0.2980\n",
            "epoch: 5/10. iter: 4700/5600. loss: 0.3394\n",
            "epoch: 5/10. iter: 4800/5600. loss: 0.3502\n",
            "epoch: 5/10. iter: 4900/5600. loss: 0.3436\n",
            "epoch: 5/10. iter: 5000/5600. loss: 0.3176\n",
            "epoch: 5/10. iter: 5100/5600. loss: 0.3038\n",
            "epoch: 5/10. iter: 5200/5600. loss: 0.2932\n",
            "epoch: 5/10. iter: 5300/5600. loss: 0.3402\n",
            "epoch: 5/10. iter: 5400/5600. loss: 0.3297\n",
            "epoch: 5/10. iter: 5500/5600. loss: 0.3129\n",
            "epoch: 5/10. iter: 5600/5600. loss: 0.3214\n",
            "epoch: 6/10. iter: 100/5600. loss: 0.3406\n",
            "epoch: 6/10. iter: 200/5600. loss: 0.3237\n",
            "epoch: 6/10. iter: 300/5600. loss: 0.3313\n",
            "epoch: 6/10. iter: 400/5600. loss: 0.3403\n",
            "epoch: 6/10. iter: 500/5600. loss: 0.3450\n",
            "epoch: 6/10. iter: 600/5600. loss: 0.3195\n",
            "epoch: 6/10. iter: 700/5600. loss: 0.3588\n",
            "epoch: 6/10. iter: 800/5600. loss: 0.3002\n",
            "epoch: 6/10. iter: 900/5600. loss: 0.3479\n",
            "epoch: 6/10. iter: 1000/5600. loss: 0.3404\n",
            "epoch: 6/10. iter: 1100/5600. loss: 0.3072\n",
            "epoch: 6/10. iter: 1200/5600. loss: 0.3349\n",
            "epoch: 6/10. iter: 1300/5600. loss: 0.3123\n",
            "epoch: 6/10. iter: 1400/5600. loss: 0.3357\n",
            "epoch: 6/10. iter: 1500/5600. loss: 0.3078\n",
            "epoch: 6/10. iter: 1600/5600. loss: 0.3272\n",
            "epoch: 6/10. iter: 1700/5600. loss: 0.3027\n",
            "epoch: 6/10. iter: 1800/5600. loss: 0.3270\n",
            "epoch: 6/10. iter: 1900/5600. loss: 0.3238\n",
            "epoch: 6/10. iter: 2000/5600. loss: 0.3158\n",
            "epoch: 6/10. iter: 2100/5600. loss: 0.3314\n",
            "epoch: 6/10. iter: 2200/5600. loss: 0.3188\n",
            "epoch: 6/10. iter: 2300/5600. loss: 0.3218\n",
            "epoch: 6/10. iter: 2400/5600. loss: 0.3320\n",
            "epoch: 6/10. iter: 2500/5600. loss: 0.3104\n",
            "epoch: 6/10. iter: 2600/5600. loss: 0.3248\n",
            "epoch: 6/10. iter: 2700/5600. loss: 0.3545\n",
            "epoch: 6/10. iter: 2800/5600. loss: 0.3219\n",
            "epoch: 6/10. iter: 2900/5600. loss: 0.3138\n",
            "epoch: 6/10. iter: 3000/5600. loss: 0.2973\n",
            "epoch: 6/10. iter: 3100/5600. loss: 0.3025\n",
            "epoch: 6/10. iter: 3200/5600. loss: 0.3201\n",
            "epoch: 6/10. iter: 3300/5600. loss: 0.3348\n",
            "epoch: 6/10. iter: 3400/5600. loss: 0.3035\n",
            "epoch: 6/10. iter: 3500/5600. loss: 0.3202\n",
            "epoch: 6/10. iter: 3600/5600. loss: 0.3025\n",
            "epoch: 6/10. iter: 3700/5600. loss: 0.3137\n",
            "epoch: 6/10. iter: 3800/5600. loss: 0.3052\n",
            "epoch: 6/10. iter: 3900/5600. loss: 0.3078\n",
            "epoch: 6/10. iter: 4000/5600. loss: 0.3068\n",
            "epoch: 6/10. iter: 4100/5600. loss: 0.2941\n",
            "epoch: 6/10. iter: 4200/5600. loss: 0.3272\n",
            "epoch: 6/10. iter: 4300/5600. loss: 0.3192\n",
            "epoch: 6/10. iter: 4400/5600. loss: 0.3081\n",
            "epoch: 6/10. iter: 4500/5600. loss: 0.3472\n",
            "epoch: 6/10. iter: 4600/5600. loss: 0.3193\n",
            "epoch: 6/10. iter: 4700/5600. loss: 0.3432\n",
            "epoch: 6/10. iter: 4800/5600. loss: 0.3496\n",
            "epoch: 6/10. iter: 4900/5600. loss: 0.3275\n",
            "epoch: 6/10. iter: 5000/5600. loss: 0.3130\n",
            "epoch: 6/10. iter: 5100/5600. loss: 0.3226\n",
            "epoch: 6/10. iter: 5200/5600. loss: 0.3197\n",
            "epoch: 6/10. iter: 5300/5600. loss: 0.3085\n",
            "epoch: 6/10. iter: 5400/5600. loss: 0.3073\n",
            "epoch: 6/10. iter: 5500/5600. loss: 0.3345\n",
            "epoch: 6/10. iter: 5600/5600. loss: 0.3324\n",
            "epoch: 7/10. iter: 100/5600. loss: 0.2964\n",
            "epoch: 7/10. iter: 200/5600. loss: 0.3217\n",
            "epoch: 7/10. iter: 300/5600. loss: 0.3221\n",
            "epoch: 7/10. iter: 400/5600. loss: 0.3273\n",
            "epoch: 7/10. iter: 500/5600. loss: 0.3109\n",
            "epoch: 7/10. iter: 600/5600. loss: 0.3260\n",
            "epoch: 7/10. iter: 700/5600. loss: 0.2923\n",
            "epoch: 7/10. iter: 800/5600. loss: 0.2933\n",
            "epoch: 7/10. iter: 900/5600. loss: 0.3361\n",
            "epoch: 7/10. iter: 1000/5600. loss: 0.2889\n",
            "epoch: 7/10. iter: 1100/5600. loss: 0.3343\n",
            "epoch: 7/10. iter: 1200/5600. loss: 0.3259\n",
            "epoch: 7/10. iter: 1300/5600. loss: 0.3232\n",
            "epoch: 7/10. iter: 1400/5600. loss: 0.3125\n",
            "epoch: 7/10. iter: 1500/5600. loss: 0.3464\n",
            "epoch: 7/10. iter: 1600/5600. loss: 0.3027\n",
            "epoch: 7/10. iter: 1700/5600. loss: 0.2968\n",
            "epoch: 7/10. iter: 1800/5600. loss: 0.2988\n",
            "epoch: 7/10. iter: 1900/5600. loss: 0.2939\n",
            "epoch: 7/10. iter: 2000/5600. loss: 0.3232\n",
            "epoch: 7/10. iter: 2100/5600. loss: 0.3241\n",
            "epoch: 7/10. iter: 2200/5600. loss: 0.2923\n",
            "epoch: 7/10. iter: 2300/5600. loss: 0.2919\n",
            "epoch: 7/10. iter: 2400/5600. loss: 0.3143\n",
            "epoch: 7/10. iter: 2500/5600. loss: 0.3145\n",
            "epoch: 7/10. iter: 2600/5600. loss: 0.3424\n",
            "epoch: 7/10. iter: 2700/5600. loss: 0.3069\n",
            "epoch: 7/10. iter: 2800/5600. loss: 0.3266\n",
            "epoch: 7/10. iter: 2900/5600. loss: 0.3392\n",
            "epoch: 7/10. iter: 3000/5600. loss: 0.3283\n",
            "epoch: 7/10. iter: 3100/5600. loss: 0.3128\n",
            "epoch: 7/10. iter: 3200/5600. loss: 0.3104\n",
            "epoch: 7/10. iter: 3300/5600. loss: 0.3203\n",
            "epoch: 7/10. iter: 3400/5600. loss: 0.3152\n",
            "epoch: 7/10. iter: 3500/5600. loss: 0.3051\n",
            "epoch: 7/10. iter: 3600/5600. loss: 0.2757\n",
            "epoch: 7/10. iter: 3700/5600. loss: 0.3322\n",
            "epoch: 7/10. iter: 3800/5600. loss: 0.3147\n",
            "epoch: 7/10. iter: 3900/5600. loss: 0.3170\n",
            "epoch: 7/10. iter: 4000/5600. loss: 0.3016\n",
            "epoch: 7/10. iter: 4100/5600. loss: 0.3115\n",
            "epoch: 7/10. iter: 4200/5600. loss: 0.3208\n",
            "epoch: 7/10. iter: 4300/5600. loss: 0.3103\n",
            "epoch: 7/10. iter: 4400/5600. loss: 0.2857\n",
            "epoch: 7/10. iter: 4500/5600. loss: 0.3199\n",
            "epoch: 7/10. iter: 4600/5600. loss: 0.3204\n",
            "epoch: 7/10. iter: 4700/5600. loss: 0.3272\n",
            "epoch: 7/10. iter: 4800/5600. loss: 0.3034\n",
            "epoch: 7/10. iter: 4900/5600. loss: 0.3084\n",
            "epoch: 7/10. iter: 5000/5600. loss: 0.3221\n",
            "epoch: 7/10. iter: 5100/5600. loss: 0.3218\n",
            "epoch: 7/10. iter: 5200/5600. loss: 0.3161\n",
            "epoch: 7/10. iter: 5300/5600. loss: 0.3026\n",
            "epoch: 7/10. iter: 5400/5600. loss: 0.3001\n",
            "epoch: 7/10. iter: 5500/5600. loss: 0.3353\n",
            "epoch: 7/10. iter: 5600/5600. loss: 0.3410\n",
            "epoch: 8/10. iter: 100/5600. loss: 0.2858\n",
            "epoch: 8/10. iter: 200/5600. loss: 0.3050\n",
            "epoch: 8/10. iter: 300/5600. loss: 0.3035\n",
            "epoch: 8/10. iter: 400/5600. loss: 0.3341\n",
            "epoch: 8/10. iter: 500/5600. loss: 0.3145\n",
            "epoch: 8/10. iter: 600/5600. loss: 0.3259\n",
            "epoch: 8/10. iter: 700/5600. loss: 0.3383\n",
            "epoch: 8/10. iter: 800/5600. loss: 0.3140\n",
            "epoch: 8/10. iter: 900/5600. loss: 0.3465\n",
            "epoch: 8/10. iter: 1000/5600. loss: 0.3155\n",
            "epoch: 8/10. iter: 1100/5600. loss: 0.3051\n",
            "epoch: 8/10. iter: 1200/5600. loss: 0.3005\n",
            "epoch: 8/10. iter: 1300/5600. loss: 0.3430\n",
            "epoch: 8/10. iter: 1400/5600. loss: 0.3499\n",
            "epoch: 8/10. iter: 1500/5600. loss: 0.3441\n",
            "epoch: 8/10. iter: 1600/5600. loss: 0.3032\n",
            "epoch: 8/10. iter: 1700/5600. loss: 0.2844\n",
            "epoch: 8/10. iter: 1800/5600. loss: 0.3478\n",
            "epoch: 8/10. iter: 1900/5600. loss: 0.3212\n",
            "epoch: 8/10. iter: 2000/5600. loss: 0.3291\n",
            "epoch: 8/10. iter: 2100/5600. loss: 0.3374\n",
            "epoch: 8/10. iter: 2200/5600. loss: 0.3285\n",
            "epoch: 8/10. iter: 2300/5600. loss: 0.3107\n",
            "epoch: 8/10. iter: 2400/5600. loss: 0.3131\n",
            "epoch: 8/10. iter: 2500/5600. loss: 0.3222\n",
            "epoch: 8/10. iter: 2600/5600. loss: 0.3302\n",
            "epoch: 8/10. iter: 2700/5600. loss: 0.3254\n",
            "epoch: 8/10. iter: 2800/5600. loss: 0.3128\n",
            "epoch: 8/10. iter: 2900/5600. loss: 0.2968\n",
            "epoch: 8/10. iter: 3000/5600. loss: 0.3231\n",
            "epoch: 8/10. iter: 3100/5600. loss: 0.3208\n",
            "epoch: 8/10. iter: 3200/5600. loss: 0.2970\n",
            "epoch: 8/10. iter: 3300/5600. loss: 0.3201\n",
            "epoch: 8/10. iter: 3400/5600. loss: 0.2976\n",
            "epoch: 8/10. iter: 3500/5600. loss: 0.3115\n",
            "epoch: 8/10. iter: 3600/5600. loss: 0.3205\n",
            "epoch: 8/10. iter: 3700/5600. loss: 0.2960\n",
            "epoch: 8/10. iter: 3800/5600. loss: 0.3005\n",
            "epoch: 8/10. iter: 3900/5600. loss: 0.3324\n",
            "epoch: 8/10. iter: 4000/5600. loss: 0.3298\n",
            "epoch: 8/10. iter: 4100/5600. loss: 0.3115\n",
            "epoch: 8/10. iter: 4200/5600. loss: 0.3097\n",
            "epoch: 8/10. iter: 4300/5600. loss: 0.2925\n",
            "epoch: 8/10. iter: 4400/5600. loss: 0.3177\n",
            "epoch: 8/10. iter: 4500/5600. loss: 0.3279\n",
            "epoch: 8/10. iter: 4600/5600. loss: 0.3231\n",
            "epoch: 8/10. iter: 4700/5600. loss: 0.3196\n",
            "epoch: 8/10. iter: 4800/5600. loss: 0.2996\n",
            "epoch: 8/10. iter: 4900/5600. loss: 0.3188\n",
            "epoch: 8/10. iter: 5000/5600. loss: 0.3007\n",
            "epoch: 8/10. iter: 5100/5600. loss: 0.3209\n",
            "epoch: 8/10. iter: 5200/5600. loss: 0.3157\n",
            "epoch: 8/10. iter: 5300/5600. loss: 0.3008\n",
            "epoch: 8/10. iter: 5400/5600. loss: 0.3126\n",
            "epoch: 8/10. iter: 5500/5600. loss: 0.3156\n",
            "epoch: 8/10. iter: 5600/5600. loss: 0.3169\n",
            "epoch: 9/10. iter: 100/5600. loss: 0.3174\n",
            "epoch: 9/10. iter: 200/5600. loss: 0.2898\n",
            "epoch: 9/10. iter: 300/5600. loss: 0.2960\n",
            "epoch: 9/10. iter: 400/5600. loss: 0.3287\n",
            "epoch: 9/10. iter: 500/5600. loss: 0.3089\n",
            "epoch: 9/10. iter: 600/5600. loss: 0.3183\n",
            "epoch: 9/10. iter: 700/5600. loss: 0.3035\n",
            "epoch: 9/10. iter: 800/5600. loss: 0.2868\n",
            "epoch: 9/10. iter: 900/5600. loss: 0.3271\n",
            "epoch: 9/10. iter: 1000/5600. loss: 0.3164\n",
            "epoch: 9/10. iter: 1100/5600. loss: 0.3160\n",
            "epoch: 9/10. iter: 1200/5600. loss: 0.3510\n",
            "epoch: 9/10. iter: 1300/5600. loss: 0.3130\n",
            "epoch: 9/10. iter: 1400/5600. loss: 0.2857\n",
            "epoch: 9/10. iter: 1500/5600. loss: 0.3206\n",
            "epoch: 9/10. iter: 1600/5600. loss: 0.3257\n",
            "epoch: 9/10. iter: 1700/5600. loss: 0.3135\n",
            "epoch: 9/10. iter: 1800/5600. loss: 0.3201\n",
            "epoch: 9/10. iter: 1900/5600. loss: 0.2876\n",
            "epoch: 9/10. iter: 2000/5600. loss: 0.3334\n",
            "epoch: 9/10. iter: 2100/5600. loss: 0.3322\n",
            "epoch: 9/10. iter: 2200/5600. loss: 0.2866\n",
            "epoch: 9/10. iter: 2300/5600. loss: 0.3072\n",
            "epoch: 9/10. iter: 2400/5600. loss: 0.3143\n",
            "epoch: 9/10. iter: 2500/5600. loss: 0.3078\n",
            "epoch: 9/10. iter: 2600/5600. loss: 0.3035\n",
            "epoch: 9/10. iter: 2700/5600. loss: 0.3093\n",
            "epoch: 9/10. iter: 2800/5600. loss: 0.2866\n",
            "epoch: 9/10. iter: 2900/5600. loss: 0.3350\n",
            "epoch: 9/10. iter: 3000/5600. loss: 0.3379\n",
            "epoch: 9/10. iter: 3100/5600. loss: 0.3373\n",
            "epoch: 9/10. iter: 3200/5600. loss: 0.3188\n",
            "epoch: 9/10. iter: 3300/5600. loss: 0.3140\n",
            "epoch: 9/10. iter: 3400/5600. loss: 0.3279\n",
            "epoch: 9/10. iter: 3500/5600. loss: 0.3258\n",
            "epoch: 9/10. iter: 3600/5600. loss: 0.3135\n",
            "epoch: 9/10. iter: 3700/5600. loss: 0.3172\n",
            "epoch: 9/10. iter: 3800/5600. loss: 0.3276\n",
            "epoch: 9/10. iter: 3900/5600. loss: 0.3213\n",
            "epoch: 9/10. iter: 4000/5600. loss: 0.2862\n",
            "epoch: 9/10. iter: 4100/5600. loss: 0.3216\n",
            "epoch: 9/10. iter: 4200/5600. loss: 0.3163\n",
            "epoch: 9/10. iter: 4300/5600. loss: 0.3047\n",
            "epoch: 9/10. iter: 4400/5600. loss: 0.2997\n",
            "epoch: 9/10. iter: 4500/5600. loss: 0.2893\n",
            "epoch: 9/10. iter: 4600/5600. loss: 0.3161\n",
            "epoch: 9/10. iter: 4700/5600. loss: 0.2902\n",
            "epoch: 9/10. iter: 4800/5600. loss: 0.3306\n",
            "epoch: 9/10. iter: 4900/5600. loss: 0.3346\n",
            "epoch: 9/10. iter: 5000/5600. loss: 0.3209\n",
            "epoch: 9/10. iter: 5100/5600. loss: 0.3036\n",
            "epoch: 9/10. iter: 5200/5600. loss: 0.3239\n",
            "epoch: 9/10. iter: 5300/5600. loss: 0.3483\n",
            "epoch: 9/10. iter: 5400/5600. loss: 0.3314\n",
            "epoch: 9/10. iter: 5500/5600. loss: 0.3006\n",
            "epoch: 9/10. iter: 5600/5600. loss: 0.3144\n",
            "epoch: 10/10. iter: 100/5600. loss: 0.3166\n",
            "epoch: 10/10. iter: 200/5600. loss: 0.3298\n",
            "epoch: 10/10. iter: 300/5600. loss: 0.3265\n",
            "epoch: 10/10. iter: 400/5600. loss: 0.2998\n",
            "epoch: 10/10. iter: 500/5600. loss: 0.3308\n",
            "epoch: 10/10. iter: 600/5600. loss: 0.3071\n",
            "epoch: 10/10. iter: 700/5600. loss: 0.3302\n",
            "epoch: 10/10. iter: 800/5600. loss: 0.2971\n",
            "epoch: 10/10. iter: 900/5600. loss: 0.3054\n",
            "epoch: 10/10. iter: 1000/5600. loss: 0.3155\n",
            "epoch: 10/10. iter: 1100/5600. loss: 0.3081\n",
            "epoch: 10/10. iter: 1200/5600. loss: 0.2897\n",
            "epoch: 10/10. iter: 1300/5600. loss: 0.2916\n",
            "epoch: 10/10. iter: 1400/5600. loss: 0.3080\n",
            "epoch: 10/10. iter: 1500/5600. loss: 0.3166\n",
            "epoch: 10/10. iter: 1600/5600. loss: 0.3337\n",
            "epoch: 10/10. iter: 1700/5600. loss: 0.3183\n",
            "epoch: 10/10. iter: 1800/5600. loss: 0.3039\n",
            "epoch: 10/10. iter: 1900/5600. loss: 0.3362\n",
            "epoch: 10/10. iter: 2000/5600. loss: 0.3392\n",
            "epoch: 10/10. iter: 2100/5600. loss: 0.2979\n",
            "epoch: 10/10. iter: 2200/5600. loss: 0.3343\n",
            "epoch: 10/10. iter: 2300/5600. loss: 0.3165\n",
            "epoch: 10/10. iter: 2400/5600. loss: 0.2808\n",
            "epoch: 10/10. iter: 2500/5600. loss: 0.2978\n",
            "epoch: 10/10. iter: 2600/5600. loss: 0.3300\n",
            "epoch: 10/10. iter: 2700/5600. loss: 0.2998\n",
            "epoch: 10/10. iter: 2800/5600. loss: 0.3276\n",
            "epoch: 10/10. iter: 2900/5600. loss: 0.3111\n",
            "epoch: 10/10. iter: 3000/5600. loss: 0.2892\n",
            "epoch: 10/10. iter: 3100/5600. loss: 0.3112\n",
            "epoch: 10/10. iter: 3200/5600. loss: 0.3153\n",
            "epoch: 10/10. iter: 3300/5600. loss: 0.3179\n",
            "epoch: 10/10. iter: 3400/5600. loss: 0.3033\n",
            "epoch: 10/10. iter: 3500/5600. loss: 0.3227\n",
            "epoch: 10/10. iter: 3600/5600. loss: 0.3048\n",
            "epoch: 10/10. iter: 3700/5600. loss: 0.2975\n",
            "epoch: 10/10. iter: 3800/5600. loss: 0.3064\n",
            "epoch: 10/10. iter: 3900/5600. loss: 0.2968\n",
            "epoch: 10/10. iter: 4000/5600. loss: 0.3110\n",
            "epoch: 10/10. iter: 4100/5600. loss: 0.3153\n",
            "epoch: 10/10. iter: 4200/5600. loss: 0.3277\n",
            "epoch: 10/10. iter: 4300/5600. loss: 0.2838\n",
            "epoch: 10/10. iter: 4400/5600. loss: 0.3169\n",
            "epoch: 10/10. iter: 4500/5600. loss: 0.3008\n",
            "epoch: 10/10. iter: 4600/5600. loss: 0.2799\n",
            "epoch: 10/10. iter: 4700/5600. loss: 0.2844\n",
            "epoch: 10/10. iter: 4800/5600. loss: 0.3123\n",
            "epoch: 10/10. iter: 4900/5600. loss: 0.3173\n",
            "epoch: 10/10. iter: 5000/5600. loss: 0.3394\n",
            "epoch: 10/10. iter: 5100/5600. loss: 0.2733\n",
            "epoch: 10/10. iter: 5200/5600. loss: 0.2994\n",
            "epoch: 10/10. iter: 5300/5600. loss: 0.2929\n",
            "epoch: 10/10. iter: 5400/5600. loss: 0.3151\n",
            "epoch: 10/10. iter: 5500/5600. loss: 0.3425\n",
            "epoch: 10/10. iter: 5600/5600. loss: 0.3159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Dataset instance for validation data."
      ],
      "metadata": {
        "id": "ufsazaN7nINV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = CollisionData(train=False)\n",
        "len(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzscVI-r_fbj",
        "outputId": "8fd3dc27-f7ff-4877-9a89-653136dfb1e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1400000"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a DataLoader instance for validation data."
      ],
      "metadata": {
        "id": "3JlAljuZnPHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_loader = DataLoader(dataset = valid_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PXDYglHL_zDG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagation of the validation features to get the predicted probabilities(of signal process)"
      ],
      "metadata": {
        "id": "FxXYdouLnUV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probabilities = []\n",
        "y_valid = []\n",
        "with torch.no_grad():\n",
        "    for features, true_labels in valid_loader:\n",
        "        predicted_probabilities += model(features)\n",
        "        y_valid += true_labels"
      ],
      "metadata": {
        "id": "SHSdvfGVAGCH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each element in the list of predicted_probabilities and y_valid is a tensor. Therefore we extract the values out of the tensor using python's list comprehension."
      ],
      "metadata": {
        "id": "HM9wJeqyojHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probabilities = [p.item() for p in predicted_probabilities]\n",
        "y_valid = [y.item() for y in y_valid]"
      ],
      "metadata": {
        "id": "cOTJ3YThBpQn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the area under the Receiver Operating Characteristic curve (roc-auc curve) for the validation predictions."
      ],
      "metadata": {
        "id": "Yd2yQxSpo7o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_valid, predicted_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGPKxR3BP_U",
        "outputId": "21297676-67c0-4a85-ced2-0ec0cdc0d353"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9376359861741399"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the test data with masses from the set {500, 750, 1250, 1500} as numpy array. The first row contains the labels so we skip it."
      ],
      "metadata": {
        "id": "uCRWpUiTpX4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.loadtxt(test_data_path, delimiter=',', skiprows=1, dtype=np.float32)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1UYfoATWR2u",
        "outputId": "51093e78-e1a8-4aa4-ad00-e583784f609d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3500000, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the test data into features and labels."
      ],
      "metadata": {
        "id": "LnaRFIkhqMqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_numpy = test_data[:, 1:]\n",
        "y_test_numpy = test_data[:, 0]"
      ],
      "metadata": {
        "id": "XHHyxAw7XMLM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the mass feature of the test data using the standard scaler we defined for the training data."
      ],
      "metadata": {
        "id": "OCsSNCeMqQPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_masses = X_test_numpy[:, 27].reshape(-1,1)\n",
        "test_masses = mass_scaler.transform(test_masses)\n",
        "X_test_numpy[:, 27] = test_masses.flatten()"
      ],
      "metadata": {
        "id": "CNArDjI-XWut"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom Dataset for test data."
      ],
      "metadata": {
        "id": "3qdYnq3jqa0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCollisionData(Dataset):\n",
        "    def __init__(self):\n",
        "        self.n_samples = y_test_numpy.shape[0]\n",
        "        self.X = torch.from_numpy(X_test_numpy)\n",
        "        self.y = torch.from_numpy(y_test_numpy).view(self.n_samples, 1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "leeiygIcYGAl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an instance of the test dataset."
      ],
      "metadata": {
        "id": "X9gObzLuqgIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TestCollisionData()"
      ],
      "metadata": {
        "id": "zfhdoXwkY6FN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an instance of test loader for the test data. It shall provide test data in batches of size 1000."
      ],
      "metadata": {
        "id": "NRVrvPa4qjsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(dataset = test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "NQDd7b9UYzy1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our trained Neural Network model, we calculate the probabilities of signal processes for the test data. We provide one batch of test data at a time, and perform forward propagation on the batch to get the probabilities."
      ],
      "metadata": {
        "id": "il1XYU--qt28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_probabilities = []\n",
        "y_test = []\n",
        "with torch.no_grad():\n",
        "    for features, true_labels in test_loader:\n",
        "        test_probabilities += model(features)\n",
        "        y_test += true_labels"
      ],
      "metadata": {
        "id": "Kt-q5k2DY_ro"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the numerical values out of tensors from test_probabilities list and y_test list."
      ],
      "metadata": {
        "id": "2LAMzvNyrMkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_probabilities = [p.item() for p in test_probabilities]\n",
        "y_test = [y.item() for y in y_test]"
      ],
      "metadata": {
        "id": "bouqdtr6Zjvu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the area under the Receiver Operating Characteristic curve for the test predictions. Note that this test data contains data with the mass feature from the set {500, 750, 1250, 1500}."
      ],
      "metadata": {
        "id": "a1JPLSefrgVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, test_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWDq6OoqZrsD",
        "outputId": "3c7f59d9-bd3a-44dc-c245-140e248e88bc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9375304618898944"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load test data from the dataset with mass=1000. Our model was trained with data with masses from the set {500, 750, 1250, 1500}. It has never seen a data with mass=1000. But since our model is a Neural Network Classifier, it can successfully interpolate and make very good predictions of event processes from the test data with mass=1000."
      ],
      "metadata": {
        "id": "mU9jfsP1sBZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1000_data = np.loadtxt(test1000_data_path, delimiter=',', skiprows=1, dtype=np.float32)\n",
        "print(test1000_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRHyUiUArnwa",
        "outputId": "b87e9be9-90b9-4003-f5b1-518e5c9418b6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3500000, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the test data into features and labels."
      ],
      "metadata": {
        "id": "iWoaBXRxsxxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1000_numpy = test1000_data[:, 1:]\n",
        "y_test1000_numpy = test1000_data[:, 0]"
      ],
      "metadata": {
        "id": "-8SbOLPGsojM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is explicitly mentioned that this test dataset is for data with mass=1000. So the mass feature is absent. Therefore we add a column of mass feature into the test features with the value as 1000. It is important because our model expects to see 28 features in the data including the mass feature."
      ],
      "metadata": {
        "id": "L6nqktpys12O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test1000_numpy = np.column_stack((X_test1000_numpy, np.full(X_test1000_numpy.shape[0], 1000.0).astype(np.float32)))"
      ],
      "metadata": {
        "id": "2jxOnPuetYnu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the mass feature in the test data using the same standard scaler we defined for training data."
      ],
      "metadata": {
        "id": "6zYEK3UXtduc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1000_masses = X_test1000_numpy[:, 27].reshape(-1,1)\n",
        "test1000_masses = mass_scaler.transform(test1000_masses)\n",
        "X_test1000_numpy[:, 27] = test1000_masses.flatten()"
      ],
      "metadata": {
        "id": "RK6LN657sqm_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a custom Dataset class for the test data."
      ],
      "metadata": {
        "id": "CnTV4ORytu0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Test1000CollisionData(Dataset):\n",
        "    def __init__(self):\n",
        "        self.n_samples = y_test1000_numpy.shape[0]\n",
        "        self.X = torch.from_numpy(X_test1000_numpy)\n",
        "        self.y = torch.from_numpy(y_test1000_numpy).view(self.n_samples, 1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "Bdt7G824s6iv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating instances of test dataset and test dataset loader."
      ],
      "metadata": {
        "id": "b1E9q3S7ty0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1000_dataset = Test1000CollisionData()\n",
        "test1000_loader = DataLoader(dataset = test1000_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "JLQuBwBvu5JY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagating the test features into our model to get the probabilities of an event process."
      ],
      "metadata": {
        "id": "cbFtEA7qt3Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1000_probabilities = []\n",
        "y_test1000 = []\n",
        "with torch.no_grad():\n",
        "    for features, true_labels in test1000_loader:\n",
        "        test1000_probabilities += model(features)\n",
        "        y_test1000 += true_labels"
      ],
      "metadata": {
        "id": "I885tgkqvD_j"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the numerical values out the tensors from test1000_probabilities and y_test1000. "
      ],
      "metadata": {
        "id": "4i8fleBBuBv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1000_probabilities = [p.item() for p in test1000_probabilities]\n",
        "y_test1000 = [y.item() for y in y_test1000]"
      ],
      "metadata": {
        "id": "9CQxrI5Iv84U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now calculate the area under the Receiver Operating Characteristic curve (roc-auc curve) for the test data with the value of mass as 1000. Our model never saw a data with mass=1000. But since the model is a Neural Network Classifier, it successfully interpolates and makes very good predictions of signal processes. The area under the roc-auc curve obtained for test data with mass=1000 is 0.9692 which signifies a very good classification."
      ],
      "metadata": {
        "id": "e8dMPM5tuOEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test1000, test1000_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GqVLOkBwEeg",
        "outputId": "6c05ea8a-03d1-43c0-e7bf-5e945695f827"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9692344399253425"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution plot of the scaled masses in training data."
      ],
      "metadata": {
        "id": "Ep2cTSdjvEYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(X_numpy_train[:, 27])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "63AFrgdMb8-f",
        "outputId": "3b23c7cb-987f-4edb-f38f-82861fa62769"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3604bac210>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3QkV30n8O+vH+qW1GppRi1pNOPxvDRj4zgQfAa/hoAxwetAAskSsiZrwIBjCLBLwm443mQ3m3Bydjf8AUuSZRdDwLzieHl5gUASjA3GNmN7jB/j18xoxmPPQyO1RiOpW1J3q7t/+0d1Sa1Xv1TVXar7/ZwzZ/RoVd3qrvrWrXtv3RJVBRER+V+g1QUgIqLmYOATERmCgU9EZAgGPhGRIRj4RESGCLW6AOUSiYTu3Lmz1cUgItowHn/88XFV7avltZ4K/J07d+LQoUOtLgYR0YYhIi/V+lo26RARGYKBT0RkCAY+EZEhXG3DF5GTAFIACgDyqrrfzfUREdHamtFp+wZVHW/CeoiIqAI26RARGcLtwFcA/yIij4vIbau9QERuE5FDInIomUy6XBwiInO5HfivVdUrAPw6gA+LyOuWv0BV71DV/aq6v6+vpnsHiIioAa4GvqqeKf0/BuA7AK50c31ERLQ21zptRaQTQEBVU6WvbwDwCbfW16i/f+TlVX/+e1dd3OSStBbfB6oF9xPLRn0f3BylMwDgOyJir+fvVfWfXFwfERFV4Frgq+oJAK9ya/lERFQfDsskIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjIEA5+IyBAMfCIiQzDwiYgMwcAnIjKE64EvIkEReUJEvu/2uoiIaG3NqOF/FMDzTVgPERFV4Grgi8hFAN4C4AturoeIiKpzu4b/PwF8HEBxrReIyG0ickhEDiWTSZeLQ0RkLtcCX0R+A8CYqj5e6XWqeoeq7lfV/X19fW4Vh4jIeG7W8A8AeKuInATwDwCuF5Gvubg+IiKqwLXAV9X/pKoXqepOADcBuE9Vb3ZrfUREVBnH4RMRGSLUjJWo6k8A/KQZ6yIiotWxhk9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAgGPhGRIRj4RESGYOATERmCgU9EZAjXAl9EoiLyqIg8JSLPishfuLUuIiKqLuTisrMArlfVtIiEATwoIj9U1YMurpOIiNbgWuCrqgJIl74Nl/6pW+sjIqLKXG3DF5GgiDwJYAzAj1T1kVVec5uIHBKRQ8lk0s3iEBEZrabAF5Fvi8hbRKSuE4SqFlT1VwBcBOBKEbl8ldfcoar7VXV/X19fPYsnIqI61BrgnwXwewCOicj/EJFL6lmJqk4CuB/AjXWWj4iIHFJT4Kvqvar6bwFcAeAkgHtF5GEReW+pQ3YFEekTkZ7S1+0A3gTgBWeKTURE9aq5iUZEegHcAuBWAE8A+AysE8CP1viTQQD3i8jTAB6D1Yb//XWVloiIGlbTKB0R+Q6ASwB8FcBvqupI6Vd3i8ih1f5GVZ8G8GpHSklEROtW67DMz6vqD8p/ICIRVc2q6n4XykVERA6rtUnnL1f52c+dLAgREbmrYg1fRLYA2AagXUReDUBKv4oD6HC5bERE5KBqTTr/ClZH7UUAPlX28xSAP3GpTERE5IKKga+qXwbwZRF5u6p+q0llIiIiF1Rr0rlZVb8GYKeIfGz571X1U6v8GREReVC1Jp3O0v8xtwtCRETuqtak87nS/5zLnohog6t18rRPikhcRMIi8mMRSYrIzW4XjoiInFPrOPwbVHUawG/AmktnCMAfu1UoIiJyXq2Bbzf9vAXAN1R1yqXyEBGRS2qdWuH7IvICgDkAfyAifQAy7hWLiIicVuv0yLcDuBbAflWdBzAD4G1uFoyIiJxVzzNtL4U1Hr/8b77icHmIiMgltU6P/FUAewA8CaBQ+rGCgU9EtGHUWsPfD+AyVVU3C0NERO6pdZTOMwC2uFkQIiJyV601/ASA50TkUQBZ+4eq+lZXSkVERI6rNfD/3M1CEBGR+2oKfFX9qYjsALBXVe8VkQ4AQXeLRkRETqp1Lp3fB/BNAJ8r/WgbgHvcKhQRETmv1k7bDwM4AGAaAFT1GIB+twpFRETOqzXws6qas78p3XzFIZpERBtIrYH/UxH5E1gPM38TgG8A+J57xSIiIqfVGvi3A0gCOAzgAwB+AOA/u1UoIiJyXq2jdIoicg+Ae1Q16XKZiIjIBRVr+GL5cxEZB3AEwJHS067+rDnFc1c2X8CpiVmcT2erv9gARVW8OD6DJ16+gImZXPU/ICNNzubwwrlpjE1zhnTbuakMTo7PIDNfqP7iFqpWw/8jWKNzXqOqLwKAiOwG8L9F5I9U9dNuF9AtZyfncMOnH0A6m0ciFsHH3rSv1UVqucOnp3D3oVMAgEu3dOHd1+xsbYHIk779izMYTqYBALdcuxP7BrpaXKLWUlV88aEXkc7mcdejL+P+P74OiVik1cVaVbU2/HcBeKcd9gCgqicA3Azg3W4WzG1PvDyJdDaPXYlOjKezyHr8zNwMZybnEAoILt3ShXNTrL3RSqqKM5NzuKQU8qcvzLW4RK03nckjnc1jT18nUtk8Dp/x7gMBqwV+WFXHl/+w1I4fdqdIzXF0NAUR4KpdmwEA53h5itHpDPrjEezo7cTk3DzmcjwJ0lKpTB5z8wXsHYhhU0cYozxuFipH1+zuBQAcG021sjgVVQv8Sg25G7qR9+hoCjt7O3Hx5g4AwAhrtBidzmCgK4ot8SgAngRpJXuf2NJt7ScMfODclHWVsysRQ39XBEfOpVtcorVVa8N/lYhMr/JzARB1oTxNc3Q0hb39MXS3hxENB4wPt9lcHtOZPAbiUWzpZuDT6uza7JZ4FAPxKI6MppAvFFtcqtYamc6gpz2M9rYg9g104djYBq3hq2pQVeOr/OtS1Q3bpJOZL+Dk+VlcsqULIoIt8Xbj26xHp62RSlu6o4hHQ2gPB41/T2ilc9MZdLeH0dEWwkA8iqIC4+kNfbG/buemMguVpH0DXTg2mkax6M2JCGq98apuIrJdRO4XkedE5FkR+ahb66rXieQMCkVdGF0w2B3FuakMigY/0Mu+NB+IR62TYDcv12mlc1OZhSa/ATb9Yb5QxHg6i8GFwI9hbr7g2c5s1wIfQB7Af1DVywBcDeDDInKZi+urmX3JVR74uUIRFwweez46nUE0HEA8arXyDcSjODed8WxNhZovly8imcou1GYTXW0ICIyuGIylsigqsKW7HQCwb4uVKUc82nHrWuCr6oiq/qL0dQrA87CmVW65I+dSCAUEuxKdAMA2a1jbbtfuAWAwHkUuX/RsTYWa78R4GgXVhRp+KBBAIhYxOvDL+zQAYG9/DIDVR+hFbtbwF4jITgCvBvDIKr+7TUQOicihZLI5szYcHU1jV6ITbSFr8zd3tgEALszON2X9XjSeyqKv7GaRzTHrPTl9YbZVRSKPOT1hnfzt4wUA+uNRJFPm3qk+NWe1CmzqsLo0u6JhbO2OenZopuuBLyIxAN8C8IequmLEj6reoar7VXV/X1+f28UBYF2CbtvUvvB9eziIUECQmjMz8OcLRczmCoi3L/bDd0etr0dT5tbeaKlkaQqSruji4L6e9jCmM/NQQ/u/Upm8lR/BxSi9aFMHznp0wIOrgS8iYVhh/3VV/bab66pHclltVkQQL+24JpqYyUGx9EC2v7ZH7xDZNflYZOl+Ml9QpLL5VhWrpdLZ/JLjBgD64xHPXvW4OUpHAPwdgOdV9VNuradexaJiPJ1FX9fSuS7i0RCmM2butPbO2VV2IEfCQURCAaPbZ2mp8XR2RW02XroSNHUitVQmj9iywB8o3ZDmxaseN2v4B2DNxXO9iDxZ+vdmF9dXk8m5eeSLuiLwu6JhTBvapGNfqpfX3ADrPWHgky2Zyq4It652s68E09n8iuNmIB7BbK6AtAevemqaD78RqvogrDtyPWW8FG6r1fBfKLVF2iNVTLFwqR5dei9dPBoy9kCmlZKp7JKrQGCxhm9qxSCdya94T/q7rBE7o9NZdEW9dX9qU0bpeIkdbsunL423hzFfUGTz5t0mvlrbLGC9J6YeyLRSMr1KDd/gvp5svoBcobgi1PvjVrZ4sZnL2MBfWcO3PrQpA5t1kqksIqHAwjBVWzwawth01pNtkdR8q9XwIyFz+3rSpT6/1drwAeumLK9h4JfYbZEpAztux9PZFSMNAKsNP1coYtLg+xPIMpPNYzZXWLWJIh4NY8zA4bt2Viw/CdqB78WToHmBn7Zqs8s/JHvcuYkdt8lUdkVzDoCFcfkm34FMlsV+ntUqBtaVoGnsoajL35NYJITOtqAnm7mMC/zxlDUkc3nHrF1zMXEsvtU2u1rNzW6fZeCbbnyNkVxAqa/HwBp+upQVq131DMSjnnxPjAv8ZDq76vMm20IBRMMBI8fir9Y2C5SPsfZeTYWaa+FejTVq+KMG9vWksnkEBOhoC674XV9XBEkPHjfmBX5q5U1XtriBY/Ez8wWkMivvFgTKR2B4r6ZCzbXWvRqAddzk8kXjBjykM3l0RkIIrDKMmzV8j6gW+CnDmnQqXaqHggHruaUe3HGpuZKpLAICdK6yn5g6NDO1yhh820A84sm7bY0K/HyhiInZ3JJ5dMrF282bXqFSZxxQmhd/yqwDmVZKprLojUVWrc2aevNVOrtyWgXbQDyKzHzRc3liVOBPzOSgunJIpi0WCSGdzXvurOymxXl0Vr8jsK8rsnA5T+ZaPuFgOXs0l5GBv8Zx02+PxffYe2JU4I+tcZetLRYJoVBUZObNudt2oW12jZpKIhbBeQa+8VabcNBmNwea9Gzboqo1rcIax419cvTarJlGBX5yjXl0bHbopbLmtOOvNa2CLRFrw3javBEYtFQytfroNsAa4dbRFlzoDzJBJldAQXXN46avy3pIjNeujs0K/FK49a9ZU7Euz7w4y51bxtNZbOoIIxhYfcK4RCyCzHwRM7lCk0tGXqGqSFao4QPWfmJS4Ns3Xa1Vw+/ttN4rr131GBn4a9VUOiPWeNqZrDnhVmnUErD4Xo177NKUmmdqbh7zhZVTipezrwRNkVpjHh1bd3sYoYB4rjnUqMAfT1s3GLWvcqMEsNiskTZoaGa1wO8tPdvWpIOZllpr/qlyvbEIxlPeqs26KV1q9l1rsEMgIOj14EnQqMBPprJIVNhpOyMhCMxq0kmm1x59AZTV8D12aUrNsxD4VfaT8zPeCjc3LUyctkYNH7Cbubx13BgX+JV22oAIOkpDM02gqhhP5SrW3Ozfea2mQs1TbbADAPTF2jAxk0OhaEbnfjqTRyggiITWjlAv9muYFfhVOp4Aa6rTtMdulnDLTK6AufnCmn0aALC5k006pqulSSfRFUFRrXtdTGA/vLzS0/ESsYjn+r7MCvwq7dXA4s1XJqjlQA6Xpldg4Jsrmc6iLRhYmD11NYtNf2bsJ6lVnmW7XKKrDePpnKeGNBsT+PYkYVUDP8rAX860DjlaKrnGlOLlTAv8dCa/6pTi5fpiEeQK3ppewZjAt3fERGnUyVo624IM/GUSsTajOuRoqWqDHYDF48qUwE9l5tecOM3mxZOgMYFfa7jFotbDzGcMCP3Fk2C1g9l7ow2oeaoNdgCsq0AARlwJzheKmM0V1hyDb/PiPSzmBX4sWvF1i/OCeOdDcksylUUwINjUUfmqx4udT9Q8lebRscWjIbQFAxg34EpwYiYHReUhmUD5PSzeOQmaE/g1DC0DzAv83s62NadVsPV1RZDK5pGZN+cOZLLkC0Wcn6k8dBcARMS629aAGv7iDLNs0vEse0fsrdKGb1+meems7JZahqkCQC+HZhprYrbylOLlEl3eG3fuhsVnSFTutN3c2YaAwFPTKxgT+Ml0Bps6wggHK2+ySTX8Wi7VgcWaynkDToK01GJTaOWKEuDNG43cUGsNPxgQbO5sQ9JDx405gV/DGHxgcQI1Uy5Nq3XYAlgYoWHCwUxL1TrYATBnArVqz5Ao57WTIAN/mVAggPaw/+f2Lha1jho+m3RMVetgB8B+WE4ORZ9Pr5BMZRENB6q2FgAM/JapNklYuVgk5KkPyQ0LU97WUsPnBGrGsmuzia7qTTq9sQjyRcXUnL9nm02ms1XvsrV57arHiMCvZZKwcrGo/wO/1lFLABANBxGLhDz3uDZyXzJlhVtHWy3NF9ZJwe836VnvSeUOW1vCY3epGxH4tUwSVi4WCfm+g3K8jrZZwHs1FWqO8XTtFaXF57j6/9ipNgbfluiKYG6+4JkbOY0I/Ho6ngAr8L32LEqnJWu8y9Zmt8+SWZKpTNXpSGymdO4nU9maOmwB7w1pZuCvIhYNIZXx941G9b4nXut8ouaodbAD4M0bjZw2lysglc1XHZJp89pJ0LXAF5EvisiYiDzj1jpqVXfgl9orz/t4bu9kKou2UOUpb8tZU716Y6el5qllHh1bT3sYwYD4ej+xt63WJh2vNXO5WcO/E8CNLi6/ZvaHVPMoHftuWx93UtoHcqUpb8v1dkZwYXYe84WiyyUjr8jMFzBdw5TitkBA0Nvp7+kVxuy7bOvotAW805HtWuCr6gMAJtxafj1qnSTMZsLdtsl09Slvy9mvNeWJRlRWUapjP+n1edPfwl22tbbh2/eweOQk2PI2fBG5TUQOicihZDLpyjrsScICVSYJs9mB7+dOynou1YHFW+v9fDDTUvU2hQL+H801XsddtoD1xLgeDz0xruWBr6p3qOp+Vd3f19fnyjpqnSTMZn+Yfh6pU+tdtjbefGUe+7Ou5S5bW5/Pn52QTGUhAnTWcF+CzUsDHloe+M1Qz0gDwDor+/lu21qnvC3nxYc5kLvsGn4td9na7BkzvfQcVycl01ls7qg+pXg5L131GBH443VMq2DrjbX5tqZSz5S3Nq8NLyP32YHf21lfk042X/TtY0LrrTwC3npinJvDMu8C8HMAl4jIaRF5v1vrqsSeJKyeDkrA3095qmfKW1tnWxCRUICBbxB7SvG2UO0x4femv4YD3yNZUntDVJ1U9Z1uLbse9UwSVi4Ra8OJ5IxLpWqtRjrjrCca8W5bkzQaboB1Jbgr0elGsVoqmcpid53blYi1LTwxLhoOulSy2vi+SaeeScLKeamjxWn1THlbLtEV8XVHNi3VSOAvDkP0336iqnUPAAG8dQey/wO/gdosAAzEo7gwO49s3n/TK9Qz5W25/q4IxqZbv9NSc4xO19/31eehcHPa5Ow8cvki+uN1VpQ81Mzl+8Afnc4AsMKqHoPd1od6birjeJla7dxUBvFobVPelhvsjuLs1JxLpSIvKRQVo9MZbO1pr+vvemMRhAKCER8eN/a+v7W7vsAfKJ0g7CxqJd8H/tnJ0odU545rv/7sZOs/JKednZyr+/0AgMHudqQyed+OwKBF4+ks8kXFYJ37STAgGIhHfRn4I6UsqPc9GeyJlv6+9ZUl3wf+mckMejvb6u4ssWv4Iz6s0Z6drL/mBgBbPbTjkrsWKkp11mYBaz8568N9ZKTBGn5vZxvaQgGc9cBJ0PeBPzI1t3CGrcdgd3vp71v/ITltZGpu4YRWD/s98cKOS+6y93v7M6/H1p52Xzb9nZ3KIByUmp8hYRMRbOtp98RJ0PeBf3ZyDlsb2Gnb24Lo6Qj7roY/lyvgwux8g006rOGbYrEptLGKwbmpjO8eZj4yOYeBeLTmObnKDXZ746rH94E/0mDzBWDtuCM+a8Nf6Hhq4EDe0h2FCGv4JhiZyqA9HER3e23TAJfb2hPFfEF9N1Ln7FSmocojULrq8UCW+DrwpzPzSGXzDYUbYLXV+S3c7FpGI5fq4WAAfbEIa/gGODtpNYXW+ryEclt92vTXaPMwYGXJWCrT8udJ+DrwGx2hYxvsifquSce+YtnW8HvS7st+DVpqPbVZL41KcUqxqDg3lWmoogRYGVTU1g/N9HXgLwyjanTH7W7H5Ow85nL+ufnq7NQcRBbHBtdrK8fiG2FksrGOfWCxhn/GR4E/PpPFfEEbby3o8cYgEF8Hvr3DNVqbtT9cPwXc2ck5JGKRuibEKre1x+rX8Ov0twTk8kUk09m6x5vbejrCaA8HWx5uTlpv5XEhS1p8EvR14I9MzSEUkLqnVbAtDM30QGeLU0amGu/EBqzRBnPzBUzNzTtYKvKS0ekMVBsbgw9YwxAHfTYW327abfSqZ9AjVz2+DvyzkxkMxKN1Payg3GLnk3923DOTcw0fyIC/70Amy8IY/HVUDLb1tPuq09be3xutLHVGQtYw7xYfN74O/DOTcw035wDAQHcEIsCZC/4IfFXFyGTjHU/AYg2n1TUVcs967rK1eWXcuVPOTM4hGg5gU0f9w1Rtg92tv/nK14F/emIW2zY1Hm6RUBDbN3XgeDLtYKlaJ5nKYm6+gO2bG39PdidiAIATPnlPaKXjyTQCAlzc29HwMi7e3IFkKosZn8y7NDyWxu5ErKFhqraLNrXj5YlZB0tVP98Gfiozj7NTGQz1x9a1nKH+GIbH/BFux0rbsW+gq+FldHeE0dcVWVgW+c/wWBo7ejsRCTX+sI6hfmsf80tlaXgsjb0D68+Sk+dnWjoW37eBP+xAuAHA3v4YTozPoOCD28SPjaYAWNu0HkN9/jkJ0krDY2ns6VvfPmKH47HRjb+fzObyODM5h6F1vif7BmKYLyheOt+6J+n5NvDtGuh6w21Pfwy5fBGnWnwp5oRjY2nEo6GGRy3Z9g7EcHwszaGZPjRfKOLF8Zl112Z3bO5AOCg4OpZyqGStYz/qdL2tBXtLVz1HW3gS9G/gj6YQCQWwfXPj7ZDA4ofshxrtsbE09g50rasdErDek1Q2j1E+/cp3Xjo/i3xR112bDQUD2J2IYdgHNfxjpZPWegN/T18MIq296vFv4JcuSxsdkmmzP2Q/tFkPj6XXfcUD+OskSEsNl8JtvTV8ABgaiPnmuAkGBDt61/dQ9vY2axBIK696/Bv4o+vvZAGAeDSM/q7Ihg+38+ksJmZy666lAOUnwY1/uU5L2fv5etvwAWBffxdOXZjd8FOTWJ3YHQ3fnV5u30Brr3p8GfjprNXJst4OW9tQfwzDG3y0gRMjdGx9sQi628Mb/iRIKx0bS2NbTzs6I/U973g1ewdiUN34I3WGx9LrbuKyDfV34cR4GvkWjdTxZeDbQeREbdZezkbvpFzoxHbgqkdEMNTvj8t1Wmp4LI09Dh03e33Q9JfLF3Hy/Kwjxw2wOFLn5PnWDALxZeA/d3YaAHCJQzX8X97WjXQ2jyOjG7cJ45nTU4hHQ9jS4CyZy102GMezZ6ZaPr83OWcmm8eRcylcNhh3ZHk7ejvRFgrg8JkpR5bXCk+fnkShqLh8a7cjy7OvsJ8bmXZkefXyZeA/dHwcA/EIdqzjTsFy1+zpBQA8PHzekeU1m6riweFxXL27d90jdGzX7unFTK6Ap05NOrI8ar1HT04gX1QcGOp1ZHltoQBes3MTHhoed2R5rfDw8fMQAa7e7cx7cumWLnRFQ3i4Re+J7wK/UFQ8NDyO1w71ORZuF23qwI7eDjx8fGPuuC+dn8WZyTn86t6EY8u8Zk8vRIAHN/DBTEs9PDyOtmAA+3dsdmyZB4YSeOFcCsnUxhzC+9DwOC4bjGNTZ5sjywsFA7h2Ty9+dmy8JU3Evgv8Z89OYXJ2Hq/b51y4AcC1exJ45MREyzpb1sMO5QNDzr0nPR1tuHxr94a96qGVHho+jyt29KC9rfEpFZZ7bWmf24iVpblcAU+8POnocQMAv7q3D2cm5/DiePPvuPVd4P/smPPhBlhNGKlsfkO2Rz54bBzbetqxK7G+ccTLXTvUiydOXcBszh8TZJnsfDqL50amFwLaKb+0tRs9HWE8eGzjBf7jL11ArlBcaNJ1in2l3YqrY98F/oPHxvGKwTgSsfVNH7Cc/aE/cHRj7bj5QhEPHx/HgSHn2u9tB/YkMF9QHDzBWv5GZ4fPtQ4HfjAguHZPLx4cbk0Txno8cCyJUEBw5U7nmrgAqzN7++b2lmSJrwJ/eCyNgy+ex5te0e/4shOxCA4M9eLrj7yEbH7j3EjyvafPYjqTxw2XbXF82Vfu2oxErA13PvyS48um5lFV3PnwSWzraccrtzkzGqXcDZdtwchUBj9+fszxZbslnc3j7sdO4Q2X9jtyT8Jyv/aKAfz06FjTnyvhq8D/m/uOIRoK4j3X7nRl+X/w+iGMpbL41uNnXFm+0wpFxd/eN4xLt3Th+kudPwlGw0G877W78MDRJJ4+zdE6G9XBExN44uVJfPD1uxEKOh8Jb3nlIC7e3IG/vu/Yhqnl3/XIy5iam8eHrtvjyvJv/dXdAIDP3j/syvLX4pvAHx5L4XtPncW7r9mBXoebc2wHhnrxyou68bkHjm+I28W///RZHE/O4CPXDyGwzjmF1vKuq3egKxrCZ+7dOAczLSoWFZ/58VEkYhG8Y/92V9YRDgbwoev24OnTU7jvBe/X8lOZeXz+Zydwze5evPriTa6sY1tPO96xfzv+76FTOH2heTdhuRr4InKjiBwRkWERud2t9ZyamMUtX3oMsUgIv/+63W6tBiKC/3jDJXh5Yha3ffWQp5t2HjlxHrd/6zAuG4zj1y8fdG09XdEwPnTdEH78whj+8h+fZ+hvIMWi4k/vOYyDJybw0TcOIRp2bnTOcv/6iouwO9GJP7z7SU/fuzGby+N9dz6GiZkcPnbDPlfX9eE3DCEcDOA9X3wUo9PNedat841TJSISBPC/ALwJwGkPGt4AAAaWSURBVGkAj4nId1X1OSfXc2Emh5vuOIh0No+v3XqV4521y71uXx/+6u2vxMe/+TTe/Jmf4ZYDu/CanZswGG9HtC2AtmDA8c7RWuTyRUzO5jA8lsY/Hh7BNx8/je2bO/Dl91257hlDq/ng63djdDqDv3vwRRw+M4Wbr96By7fG0RuLoKMtiLALzQRUP1XFbK6Ac9MZPH16El/5+Ut44uVJfPgNe3Dz1TtcXXdbKICv3noV3nnHQdx0x0H8m9dsx42Xb8Huvk50RcKIhAKuXYVWUiwq0rk8Tk3M4rEXJ/Clh0/i1MQs/uadV+A1DnfWLretpx13vvdKvPdLj+KmOw7ie//utYi50F9Qzs2lXwlgWFVPAICI/AOAtwFwNPB7OsL43f3b8cZX9ONyFzqcVvO7+7djU0cbPv2jo/gv9zyz5HfBgCAcXHvHFVT43Rq/UgUUWvofwLLvVRXlD+RqCwXwW7+yFR+/8VLXT4BWuQX/9Tcvw47eDnz+gRP493c9seT34aBUPOlUek+s5ddfpkoXGoq1f1n57+r9hQvrAipeRVX+u6XfX7y5A//tt38Z77xye1MqKdt62nH3B67GJ//pCL7+yEu48+GTS37fFgqgUuY3cuwAi8dPUbFw7BR15XEDAK/a3oNPvO1yvH5fX/UNcsCVuzbjK++/Co+dnHA97AFA3LoEF5HfAXCjqt5a+v5dAK5S1Y8se91tAG4rfXsJgCNlv04A2FjjIGvj1+0C/Ltt3K6Nx6/btny7dqhqTWco908pVajqHQDuWO13InJIVfc3uUiu8+t2Af7dNm7XxuPXbVvPdrnZuHoGQHm3/0WlnxERUQu4GfiPAdgrIrtEpA3ATQC+6+L6iIioAteadFQ1LyIfAfDPAIIAvqiqz9a5mFWbenzAr9sF+HfbuF0bj1+3reHtcq3TloiIvIUDpImIDMHAJyIyhKcCX0TeISLPikhRRNYcdiQiJ0XksIg8KSKHmlnGRtSxXU2ZisJJIrJZRH4kIsdK/686+YiIFEqf15Mi4tnO+2qfgYhEROTu0u8fEZGdzS9l/WrYrltEJFn2Gd3ainLWS0S+KCJjIvLMGr8XEfnr0nY/LSJXNLuMjahhu64Tkamyz+vPalqwqnrmH4BXwLr56icA9ld43UkAiVaX18ntgtWxfRzAbgBtAJ4CcFmry17Dtn0SwO2lr28H8FdrvC7d6rLWsC1VPwMAHwLwf0pf3wTg7laX26HtugXA37a6rA1s2+sAXAHgmTV+/2YAPwQgAK4G8Eiry+zQdl0H4Pv1LtdTNXxVfV5Vj1R/5cZS43YtTEWhqjkA9lQUXvc2AF8uff1lAL/VwrKsVy2fQfn2fhPAG6UVkyfVZ6PuW1Wp6gMAJiq85G0AvqKWgwB6RMS92QQdUsN2NcRTgV8HBfAvIvJ4aWoGP9gG4FTZ96dLP/O6AVUdKX19DsDAGq+LisghETkoIl49KdTyGSy8RlXzAKYAOPsMPOfVum+9vdTs8U0RcWeu5ObbqMdVLa4RkadE5Ici8ku1/EHTp1YQkXsBrPb4pT9V1f9X42Jeq6pnRKQfwI9E5IXSGbFlHNouT6q0beXfqKqKyFrjfHeUPrPdAO4TkcOqetzpslLDvgfgLlXNisgHYF3FXN/iMtHafgHrmEqLyJsB3ANgb7U/anrgq+qvObCMM6X/x0TkO7AuWVsa+A5sl2enoqi0bSIyKiKDqjpSulRe9QkXZZ/ZCRH5CYBXw2pX9pJaPgP7NadFJASgG4DXH+pbdbtUtXwbvgCrb8YPPHtcrYeqTpd9/QMR+ayIJFS14mRxG65JR0Q6RaTL/hrADQBW7cneYDbqVBTfBfCe0tfvAbDiakZENolIpPR1AsABODxNtkNq+QzKt/d3ANynpV40D6u6Xcvatd8K4Pkmls9N3wXw7tJonasBTJU1QW5YIrLF7jsSkSthZXn1ikere6OX9Tz/Nqw2tiyAUQD/XPr5VgA/KH29G9Yog6cAPAuryaTlZV/vdpW+fzOAo7Bqvp7frlKZewH8GMAxAPcC2Fz6+X4AXyh9fS2Aw6XP7DCA97e63BW2Z8VnAOATAN5a+joK4BsAhgE8CmB3q8vs0Hb999Lx9BSA+wFc2uoy17hddwEYATBfOsbeD+CDAD5Y+r3AehDT8dK+t+boPy/9q2G7PlL2eR0EcG0ty+XUCkREhthwTTpERNQYBj4RkSEY+EREhmDgExEZgoFPRGQIBj4RkSEY+EREhvj/9zVHI/RiUTwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution plot of the scaled masses in the test data with mass=1000."
      ],
      "metadata": {
        "id": "TcMhM4i_vK57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(X_test1000_numpy[:, 27])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "CamejszTcvZ6",
        "outputId": "d621e917-ff05-4e1c-957c-1e1e82e63919"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:316: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f35fe68ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}